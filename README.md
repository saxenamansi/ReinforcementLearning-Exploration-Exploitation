# ReinforcementLearning-Exploration-Exploitation

The main task of a reinforcement learning agent is to develop a policy that maps states to optimal actions. The agent makes decisions from the available actions and learns which of them yield the best rewards. In this way, a Reinforcement Learning agent maximizes the reward received on each action decision by exploiting its current knowledge of actions and choosing the most seemingly favorable action. 

However, to gain knowledge on how favorable each action is, it must explore all the available actions. This is where the conflict between exploration and exploitation exists. 

There are several existing algorithms that provide a balance between this bias. Some of these techniques are the Epsilon-Greedy Approach, the Optimistically High Initial Values Approach and the Upper Confidence Bound Approach. 

This paper presents a comparative analysis of these algorithms, their advantages and disadvantages, simulated for the K-Armed Bandit problem. Based on this research, a combination of these methods is used in maximizing rewards attained in the long term. The effect of varying the epsilon parameter is analyzed by running various simulations, and the change in performance is noted when the epsilon parameter is reduced as convergence is achieved.
